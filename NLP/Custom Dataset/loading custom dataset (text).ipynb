{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e288af-b406-4ef3-ade6-9316b153c99e",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4dcc33-6e43-4fe2-abd6-72ad25c8deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "#sys libs\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#data manupulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandarallel import pandarallel\n",
    "# Initialization\n",
    "pandarallel.initialize()\n",
    "\n",
    "\n",
    "#string manupulation libs\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "import spacy\n",
    "\n",
    "#torch libs\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16969fa9-92d4-4c54-9076-6164ff8aaf20",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982eec0e-e243-46b0-a90e-c381cfea6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Hindi_English_Truncated_Corpus.csv')\n",
    "data = data.reset_index(drop=True)\n",
    "data.drop('source',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7defc651-8436-48b0-a140-95333db6cdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess\n",
    "\n",
    "data = data.dropna().drop_duplicates()\n",
    "\n",
    "#lower and remove quotes\n",
    "data['english_sentence'] = data.english_sentence.parallel_apply(lambda x: re.sub(\"'\", '',x).lower())\n",
    "data['hindi_sentence'] = data.hindi_sentence.parallel_apply(lambda x: re.sub(\"'\", '', x).lower())\n",
    "    \n",
    "#remove special chars\n",
    "exclude = set(string.punctuation)#set of all special chars\n",
    "#remove all the special chars\n",
    "data['english_sentence'] = data.english_sentence.parallel_apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "data['hindi_sentence'] = data.hindi_sentence.parallel_apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "    \n",
    "remove_digits = str.maketrans('','',digits)\n",
    "data['english_sentence'] = data.english_sentence.parallel_apply(lambda x: x.translate(remove_digits))\n",
    "data['hindi_sentence'] = data.hindi_sentence.parallel_apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "data['hindi_sentence'] = data.hindi_sentence.parallel_apply(lambda x: re.sub(\"[२३०८१५७९४६]\",\"\",x))\n",
    "\n",
    "\n",
    "# Remove extra spaces\n",
    "data['english_sentence']=data['english_sentence'].parallel_apply(lambda x: x.strip())\n",
    "data['hindi_sentence']=data['hindi_sentence'].parallel_apply(lambda x: x.strip())\n",
    "data['english_sentence']=data['english_sentence'].parallel_apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "data['hindi_sentence']=data['hindi_sentence'].parallel_apply(lambda x: re.sub(\" +\", \" \", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b9ce2e-6771-454f-b721-b5091cba6285",
   "metadata": {},
   "source": [
    "## 2. Create Dataset and Dataloaders\n",
    "* We create a dataset for train. \n",
    "* The vocab will only be based upon train.\n",
    "* We'll have to write separate prediction code for val and test\n",
    "\n",
    "### 2.1 Create Train and Val dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf2ce32-82b8-4eb7-9601-b743b420f8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train:  112343\n",
      "len of val:  12482\n"
     ]
    }
   ],
   "source": [
    "val_frac = 0.1 #precentage data in val\n",
    "val_split_idx = int(len(data)*val_frac) #index on which to split\n",
    "data_idx = list(range(len(data))) #create a list of ints till len of data\n",
    "np.random.shuffle(data_idx)\n",
    "\n",
    "#get indexes for validation and train\n",
    "val_idx, train_idx = data_idx[:val_split_idx], data_idx[val_split_idx:]\n",
    "print('len of train: ', len(train_idx))\n",
    "print('len of val: ', len(val_idx))\n",
    "\n",
    "#create the sets\n",
    "train = data.iloc[train_idx].reset_index().drop('index',axis=1)\n",
    "val = data.iloc[val_idx].reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8eb1e-ebbc-4ad0-a29a-8e39bacfdcdb",
   "metadata": {},
   "source": [
    "We have the dataframes now. Next we need to create Datasets and Dataloaders for these dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d7568b-3549-4fda-a5d9-c13c2422dc84",
   "metadata": {},
   "source": [
    "### 2.2 Create Datasets\n",
    "* We'll create the main vocab using train and create train_dataset\n",
    "* We'll use the train vocab for val and create val_dataset as we want to see how the model handles oov words\n",
    "* We'll not touch the test set. We'll use this set like in production. We'll pass one test sentence to the model and it will give us a translated sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fe6ba-8f2d-4ffa-9b05-f8a1d0aaddd5",
   "metadata": {},
   "source": [
    "#### 2.2.1 Create the Vocabulary Class\n",
    "\n",
    "The goals of Vocabulary class are:\n",
    "1. to create a dict mapping from string->index and index->string with pre defined size\n",
    "2. convert the tokenized text into its corresponding number representation. Eg: ['i', 'love', 'apple'] -> [23, 54, 1220]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be08f0a-c3c5-4426-b9a4-d1f36a9ecdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \n",
    "    '''\n",
    "    __init__ method is called by default as soon as an object of this class is initiated\n",
    "    we use this method to initiate our vocab dictionaries\n",
    "    '''\n",
    "    def __init__(self, freq_threshold, max_size):\n",
    "        '''\n",
    "        freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "        max_size : max source vocab size\n",
    "        '''\n",
    "        #initiate the index to token dict\n",
    "        self.itos = {0: '<PAD>', 1:'<SOS>', 2:'<EOS>', 3: '<UNK>'}\n",
    "        #initiate the token to index dict\n",
    "        self.stoi = {k:j for j,k in self.itos.items()} \n",
    "        \n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "    \n",
    "    '''\n",
    "    __len__ is used by dataloader later to create batches\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    '''\n",
    "    a simple tokenizer to split on space and converts the sentence to list of words\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower().strip() for tok in text.split(' ')]\n",
    "    \n",
    "    '''\n",
    "    build the vocab: create a dictionary mapping of index to string (itos) and string to index (stoi)\n",
    "    output ex. for stoi -> {'the':5, 'a':6, 'an':7}\n",
    "    '''\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        #calculate the frequencies of each word first to remove the words with freq < freq_threshold\n",
    "        frequencies = {}  #init the freq dict\n",
    "        idx = 4 #index from which we want our dict to start. We already used 4 indexes for pad, start, end, unk\n",
    "        \n",
    "        #calculate freq of words\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "                    \n",
    "                    \n",
    "        #limit vocab by removing low freq words\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold} \n",
    "        \n",
    "        #limit vocab to the max_size specified\n",
    "        if len(frequencies)>self.max_size-idx: \n",
    "            frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-idx]) # idx =4 for pad, start, end , unk\n",
    "            \n",
    "        #create vocab\n",
    "        for word in frequencies.keys():\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx+=1\n",
    "            \n",
    "    '''\n",
    "    convert the list of words to a list of corresponding indexes\n",
    "    '''    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        numericalized_text = []\n",
    "        for token in tokenized_text:\n",
    "            if token in self.stoi.keys():\n",
    "                numericalized_text.append(self.stoi[token])\n",
    "            else: #out-of-vocab words are represented by UNK token index\n",
    "                numericalized_text.append(self.stoi['<UNK>'])\n",
    "                \n",
    "        return numericalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab702894-e59a-49bc-b8ac-7fa65453c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index to string:  {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>', 4: 'that', 5: 'is', 6: 'a', 7: 'cat', 8: 'not', 9: 'dog'}\n",
      "string to index: {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'that': 4, 'is': 5, 'a': 6, 'cat': 7, 'not': 8, 'dog': 9}\n",
      "numericalize -> cat and a dog:  [7, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "#create a vocab class with freq_threshold=0 and max_size=100\n",
    "voc = Vocabulary(0, 100)\n",
    "sentence_list = ['that is a cat', 'that is not a dog']\n",
    "#build vocab\n",
    "voc.build_vocabulary(sentence_list)\n",
    "\n",
    "print('index to string: ',voc.itos)\n",
    "print('string to index:',voc.stoi)\n",
    "\n",
    "print('numericalize -> cat and a dog: ', voc.numericalize('cat and a dog'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca27e32-7feb-41fa-9c6d-dc95ac1599ae",
   "metadata": {},
   "source": [
    "#### 2.2.2 Create the Train Dataset class\n",
    "1. For Train Dataset class, we first inherit the pytorch's Dataset class.\n",
    "2. Then, we initialize and build the vocabs for both source and target columns in our train dataframe.\n",
    "3. Then, we use getitem() method to numericalize the text 1 example at a time for the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3fa2608-b076-4d86-85b3-6fc2ee005b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    '''\n",
    "    Initiating Variables\n",
    "    df: the training dataframe\n",
    "    source_column : the name of source text column in the dataframe\n",
    "    target_columns : the name of target text column in the dataframe\n",
    "    transform : If we want to add any augmentation\n",
    "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "    source_vocab_max_size : max source vocab size\n",
    "    target_vocab_max_size : max target vocab size\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, df, source_column, target_column, transform=None, freq_threshold = 5,\n",
    "                source_vocab_max_size = 10000, target_vocab_max_size = 10000):\n",
    "    \n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "        #get source and target texts\n",
    "        self.source_texts = self.df[source_column]\n",
    "        self.target_texts = self.df[target_column]\n",
    "        \n",
    "        \n",
    "        ##VOCAB class has been created above\n",
    "        #Initialize source vocab object and build vocabulary\n",
    "        self.source_vocab = Vocabulary(freq_threshold, source_vocab_max_size)\n",
    "        self.source_vocab.build_vocabulary(self.source_texts.tolist())\n",
    "        #Initialize target vocab object and build vocabulary\n",
    "        self.target_vocab = Vocabulary(freq_threshold, target_vocab_max_size)\n",
    "        self.target_vocab.build_vocabulary(self.target_texts.tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    '''\n",
    "    __getitem__ runs on 1 example at a time. Here, we get an example at index and return its numericalize source and\n",
    "    target values using the vocabulary objects we created in __init__\n",
    "    '''\n",
    "    def __getitem__(self, index):\n",
    "        source_text = self.source_texts[index]\n",
    "        target_text = self.target_texts[index]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            source_text = self.transform(source_text)\n",
    "            \n",
    "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
    "        numerialized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_source += self.source_vocab.numericalize(source_text)\n",
    "        numerialized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "    \n",
    "        numerialized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_target += self.target_vocab.numericalize(target_text)\n",
    "        numerialized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        #convert the list to tensor and return\n",
    "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fabf3d5-4e51-4f0f-81b3-cc1cec7757d2",
   "metadata": {},
   "source": [
    "#### 2.2.3 Create the Validation Dataset class\n",
    "\n",
    "Create a separate dataset for validation as we\n",
    "are going to reuse train's vocabulary.\n",
    "\n",
    "Better practice to define a separate class for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aebbc5a-d7a4-4fe4-ac8d-fbcc94f2947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validation_Dataset:\n",
    "    def __init__(self, train_dataset, df, source_column, target_column, transform = None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "        #train dataset will be used as lookup for vocab\n",
    "        self.train_dataset = train_dataset\n",
    "        \n",
    "        #get source and target texts\n",
    "        self.source_texts = self.df[source_column]\n",
    "        self.target_texts = self.df[target_column]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        source_text = self.source_texts[index]\n",
    "        #print(source_text)\n",
    "        target_text = self.target_texts[index]\n",
    "        #print(target_text)\n",
    "        if self.transform is not None:\n",
    "            source_text = self.transform(source_text)\n",
    "            \n",
    "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
    "        numerialized_source = [self.train_dataset.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_source += self.train_dataset.source_vocab.numericalize(source_text)\n",
    "        numerialized_source.append(self.train_dataset.source_vocab.stoi[\"<EOS>\"])\n",
    "    \n",
    "        numerialized_target = [self.train_dataset.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_target += self.train_dataset.target_vocab.numericalize(target_text)\n",
    "        numerialized_target.append(self.train_dataset.target_vocab.stoi[\"<EOS>\"])\n",
    "        #print(numerialized_source)\n",
    "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e4366-750d-461b-ba5f-8f9ced19d580",
   "metadata": {},
   "source": [
    "Let us test some examples on our Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc272635-7684-4162-9cfb-47ff20c7394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_sentence    narrowed or blocked arteries are bypassed by t...\n",
      "hindi_sentence      संकुचित हो गयी या अवरोधित धमनियों को टाँग से ल...\n",
      "Name: 1, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([   1,    3,   24, 8001, 5987,   17,    3,   20,    4,  151,    5,    9,\n",
       "         7360,    5,    3,  256,   23,    4, 1826,   27,   10, 1120,    7,    4,\n",
       "            3,   31,   39,  189,    6,    7,    4,    3, 7360,  918,    4, 2794,\n",
       "           16,    4,   52,    2]),\n",
       " tensor([   1, 6701,   25,  162,   31,    3, 4979,   11,    3,    9,   19,  112,\n",
       "            3,    4, 3555,   10,  169,   23,    3,   23,   51,   42,    3,    4,\n",
       "           20, 3555,    4,   13, 3598,   11,    3,    9,    8,  174, 3598,   11,\n",
       "         4980,    4,  338, 1821,   51,   42,    6,    2]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Train_Dataset(train, 'english_sentence', 'hindi_sentence')\n",
    "print(train.loc[1])\n",
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9b258-073f-4558-928e-3b8db47b1ea4",
   "metadata": {},
   "source": [
    "Cool! We get the indexes for both the source and target sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5055a8-1d03-4fbd-a4e7-de891af02d51",
   "metadata": {},
   "source": [
    "#### 2.2.4 DataLoaders\n",
    "First we learn about collate fn. This function is created to run AFTER the batch has been created using the dataloader. We use this function to create padding for the sentences in our batch. We use \"pad_sequence\" for this. It picks the largest sentence in our batch and pads all other sentences to this length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f8125d8-219e-4b59-a2e3-c84938cacbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class to add padding to the batches\n",
    "collat_fn in dataloader is used for post processing on a single batch. Like __getitem__ in dataset class\n",
    "is used on single example\n",
    "'''\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    \n",
    "    #__call__: a default method\n",
    "    ##   First the obj is created using MyCollate(pad_idx) in data loader\n",
    "    ##   Then if obj(batch) is called -> __call__ runs by default\n",
    "    def __call__(self, batch):\n",
    "        #get all source indexed sentences of the batch\n",
    "        source = [item[0] for item in batch] \n",
    "        #pad them using pad_sequence method from pytorch. \n",
    "        source = pad_sequence(source, batch_first=False, padding_value = self.pad_idx) \n",
    "        \n",
    "        #get all target indexed sentences of the batch\n",
    "        target = [item[1] for item in batch] \n",
    "        #pad them using pad_sequence method from pytorch. \n",
    "        target = pad_sequence(target, batch_first=False, padding_value = self.pad_idx)\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd50fec-517c-4298-b682-520e16aec020",
   "metadata": {},
   "source": [
    "Now we define the data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9776e34a-de8a-4ad8-ba1f-c231c667c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we run a next(iter(data_loader)) we get an output of batch_size * (num_workers+1)\n",
    "def get_train_loader(dataset, batch_size, num_workers=0, shuffle=True, pin_memory=False):\n",
    "    #get pad_idx for collate fn\n",
    "    pad_idx = dataset.source_vocab.stoi['<PAD>']\n",
    "    #define loader\n",
    "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
    "                        shuffle=shuffle,\n",
    "                       pin_memory=pin_memory, collate_fn = MyCollate(pad_idx=pad_idx))\n",
    "    return loader\n",
    "\n",
    "def get_valid_loader(dataset, train_dataset, batch_size, num_workers=1, shuffle=True, pin_memory=False):\n",
    "    pad_idx = train_dataset.source_vocab.stoi['<PAD>']\n",
    "    loader = DataLoader(dataset, batch_size = batch_size, num_workers = num_workers,\n",
    "                        shuffle=shuffle,\n",
    "                       pin_memory=pin_memory, collate_fn = MyCollate(pad_idx=pad_idx))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1cf26c-a932-43ed-8f58-4ed877901b9e",
   "metadata": {},
   "source": [
    "Let us see the result from a dataloader. We create an iterator and get the tuple of source and target batch of size 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d023dcad-0b9f-4728-85c6-429d72100e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: \n",
      " tensor([[   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [4125,  420,   37,  ...,   46,   95, 5406],\n",
      "        [  91,   32,   17,  ..., 1407,   53,   17],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_train_loader(train_dataset, 32)\n",
    "source = next(iter(train_loader))[0]\n",
    "target = next(iter(train_loader))[1]\n",
    "\n",
    "print('source: \\n', source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2c1914d-0623-4b36-b461-108c94efc43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,    1,    1,  ...,    1,    1,    1],\n",
       "        [  33,   37, 1057,  ...,  162,   24,    6],\n",
       "        [  90,   12,  892,  ...,    7,    9,   13],\n",
       "        ...,\n",
       "        [   0,    0,    0,  ...,    0,    0,    0],\n",
       "        [   0,    0,    0,  ...,    0,    0,    0],\n",
       "        [   0,    0,    0,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4528fc5a-5e67-4ebc-8a6c-f79458aa8df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source shape:  torch.Size([64, 32])\n",
      "target shape:  torch.Size([44, 32])\n"
     ]
    }
   ],
   "source": [
    "print('source shape: ',source.shape)\n",
    "print('target shape: ', target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6200952-f005-404d-a7fe-11a27a7acc2b",
   "metadata": {},
   "source": [
    "One sentence is arranged vertically in the above tensor array. We can see the 0s which are the padding for the sentences. And 79 is longest sentence in the batch\n",
    "\n",
    "Now, the data is ready to be used in our models!\n",
    "This was part 2 of our 2 part series for loading custom data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49b9de-bae5-4f51-a3e8-541569629ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
