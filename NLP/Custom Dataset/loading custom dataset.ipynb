{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5fa02ae-019c-4b3b-89ee-e28b303f8fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandarallel import pandarallel #for parallel apply function\n",
    "\n",
    "# Initialization\n",
    "pandarallel.initialize()\n",
    "\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e6955-9a99-40d7-89a2-611a745bf547",
   "metadata": {},
   "source": [
    "### 1. Load data and preprocess\n",
    "We'll first load the data. Then we'll preprocess and clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa699a9a-c26a-4171-9d0f-6d424215d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Hindi_English_Truncated_Corpus.csv') #https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora/kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f7dd12-a749-4642-ab46-0e02f0a6da11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                   english_sentence  \\\n",
       "0        ted  politicians do not have permission to do what ...   \n",
       "1        ted         I'd like to tell you about one such child,   \n",
       "2  indic2012  This percentage is even greater than the perce...   \n",
       "3        ted  what we really mean is that they're bad at not...   \n",
       "4  indic2012  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4574e3bf-0222-4463-bf7b-6f1433b532c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 127607 entries, 0 to 127606\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   source            127607 non-null  object\n",
      " 1   english_sentence  127605 non-null  object\n",
      " 2   hindi_sentence    127607 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75a1d036-c502-43a3-b9d8-1548ab504f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124827"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocess and clean\n",
    "#clean the data\n",
    "print(len(data))\n",
    "data = data.dropna().drop_duplicates()\n",
    "\n",
    "#lower and remove quotes\n",
    "data['english_sentence'] = data.english_sentence.parallel_apply(lambda x: re.sub(\"'\",'',x).lower())\n",
    "data['hindi_sentence'] = data.hindi_sentence.parallel_apply(lambda x: re.sub(\"'\",'',x).lower())\n",
    "\n",
    "#remove special chars\n",
    "exclude = set(string.punctuation) # Set of all special characters\n",
    "# Remove all the special characters\n",
    "data['english_sentence']=data['english_sentence'].parallel_apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "data['hindi_sentence']=data['hindi_sentence'].parallel_apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "data['english_sentence']=data['english_sentence'].parallel_apply(lambda x: x.translate(remove_digits))\n",
    "data['hindi_sentence']=data['hindi_sentence'].parallel_apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "data['hindi_sentence'] = data['hindi_sentence'].parallel_apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "\n",
    "# Remove extra spaces\n",
    "data['english_sentence']=data['english_sentence'].parallel_apply(lambda x: x.strip())\n",
    "data['hindi_sentence']=data['hindi_sentence'].parallel_apply(lambda x: x.strip())\n",
    "data['english_sentence']=data['english_sentence'].parallel_apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "data['hindi_sentence']=data['hindi_sentence'].parallel_apply(lambda x: re.sub(\" +\", \" \", x))\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2427ae-de8e-40f6-a8e1-745838f29bf3",
   "metadata": {},
   "source": [
    "Great! Now we have clean text. Now, as some of the rows were dropped from our data, we'll reset the index as the dataset class reads throws error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c45e11e0-50fa-4c4b-9999-95b8bfd4c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index as some rows were dropped. Else dataset getitem will give error as it works on index\n",
    "data = data.reset_index().drop('index',axis=1)\n",
    "#dropping source column as it is not needed\n",
    "data.drop('source',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f2c732c-e425-46f9-8c50-5ac8f211a306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id like to tell you about one such child</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that theyre bad at not ...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the ending portion of these vedas is called up...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  politicians do not have permission to do what ...   \n",
       "1           id like to tell you about one such child   \n",
       "2  this percentage is even greater than the perce...   \n",
       "3  what we really mean is that theyre bad at not ...   \n",
       "4  the ending portion of these vedas is called up...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa3e3a-3d29-4f70-876f-fcea2f2b589a",
   "metadata": {},
   "source": [
    "Ok, so our data is ready. Now we'll create the custom dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4815daf-10ad-4fae-a1fd-16c50d1f2d09",
   "metadata": {},
   "source": [
    "### 2. Create the dataset class\n",
    "Takes in dataframe, source and target text columns and returns numericalized texts one index at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8afeb342-211a-45ee-ab24-6bb25f012c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_Dataset(Dataset): \n",
    "    \n",
    "    '''\n",
    "    Initiating Variables\n",
    "    source_column : the name of source text column in the dataframe\n",
    "    target_columns : the name of target text column in the dataframe\n",
    "    transform : If we want to add any augmentation\n",
    "    freq_threshold : the minimum times a word must occur in corpus to be treated in vocab\n",
    "    source_vocab_max_size : max source vocab size\n",
    "    target_vocab_max_size : max target vocab size\n",
    "    '''\n",
    "    def __init__(self, df, source_column, target_column, transform = None, freq_threshold = 5, \n",
    "                 source_vocab_max_size= 10000, target_vocab_max_size = 10000):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "        #get source and target texts\n",
    "        self.source_texts = self.df[source_column]\n",
    "        self.target_texts = self.df[target_column]\n",
    "        \n",
    "        ##VOCABULARY class will be created below\n",
    "        #create source vocab\n",
    "        self.source_vocab = Vocabulary(freq_threshold,source_vocab_max_size)\n",
    "        self.source_vocab.build_vocabulary(self.source_texts.tolist())\n",
    "        #create target vocab\n",
    "        self.target_vocab = Vocabulary(freq_threshold,target_vocab_max_size)\n",
    "        self.target_vocab.build_vocabulary(self.target_texts.tolist())\n",
    "        \n",
    "    #used by data loader when creating batches\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    #getitem gets 1 example at a time. This is done before a batch is created\n",
    "    def __getitem__(self, index):\n",
    "        source_text = self.source_texts[index]\n",
    "        #print(source_text)\n",
    "        target_text = self.target_texts[index]\n",
    "        #print(target_text)\n",
    "        if self.transform is not None:\n",
    "            source_text = self.transform(source_text)\n",
    "            \n",
    "        #numericalize texts ['<SOS>','cat', 'in', 'a', 'bag','<EOS>'] -> [1,12,2,9,24,2]\n",
    "        numerialized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_source += self.source_vocab.numericalize(source_text)\n",
    "        numerialized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "    \n",
    "        numerialized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numerialized_target += self.target_vocab.numericalize(target_text)\n",
    "        numerialized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "        #print(numerialized_source)\n",
    "        return torch.tensor(numerialized_source), torch.tensor(numerialized_target) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "189716d4-824b-422b-85be-e6b66b53a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, max_size):\n",
    "        #defining pad, start of sentence, end of sentence and unknown token index\n",
    "        self.itos = {0:\"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #index to string dict\n",
    "        self.stoi = {k:j for j,k in self.itos.items()} #string to index dict\n",
    "        self.freq_threshold = freq_threshold #minimum word frequency needed to be included in vocab\n",
    "        self.max_size = max_size #max vocab size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    a simple tokenizer that splits on space and converts the sentence to list of words \n",
    "    '''\n",
    "    @staticmethod #static method is independent of a class instance\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower().strip() for tok in text.split(' ')]\n",
    "    \n",
    "    '''\n",
    "    build the vocab\n",
    "    '''\n",
    "    def build_vocabulary(self,sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        #calculate freq of words\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word]=1\n",
    "                else:\n",
    "                    frequencies[word]+=1\n",
    "        \n",
    "        #limit vocab by removing low freq words\n",
    "        frequencies = {k:v for k,v in frequencies.items() if v>self.freq_threshold}\n",
    "        #limit vocab to the max size specified\n",
    "        if len(frequencies)>self.max_size-4:\n",
    "            frequencies = dict(sorted(frequencies.items(), key = lambda x: -x[1])[:self.max_size-4]) #-4 for start,end, pad, unk token\n",
    "        #create vocab\n",
    "        for word in frequencies.keys():\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx+=1\n",
    "                \n",
    "    '''\n",
    "    convert the list of words to a list of corresponding indexes\n",
    "    '''\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        numericalized_text = []\n",
    "        for token in tokenized_text:\n",
    "            if token in self.stoi.keys():\n",
    "                numericalized_text.append(self.stoi[token])\n",
    "            else:\n",
    "                numericalized_text.append(self.stoi['<UNK>'])\n",
    "        \n",
    "        return numericalized_text\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912deed2-b171-4e3a-a63a-aedc5c9ae144",
   "metadata": {},
   "source": [
    "> Let us test the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88354824-02fb-41f4-87a9-11f560d1df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dataset = Language_Dataset(data, 'english_sentence', 'hindi_sentence', source_vocab_max_size=10000, target_vocab_max_size=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3be906a2-a031-4f36-b3b4-42606054ecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicians do not have permission to do what needs to be done\n",
      "राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करने कि अनुमति नहीं है\n"
     ]
    }
   ],
   "source": [
    "#let us check one example by passing an index of dataframe\n",
    "print(lang_dataset.source_texts[0])\n",
    "print(lang_dataset.target_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e96f138-92f4-4237-b50e-a570cea8d727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of source vocab :  10000\n",
      "len of target vocab :  8000\n"
     ]
    }
   ],
   "source": [
    "#let us check the vocabs English and hindi\n",
    "print('len of source vocab : ',len(lang_dataset.source_vocab.stoi)) #len of the source vocab dictionary\n",
    "print('len of target vocab : ',len(lang_dataset.target_vocab.stoi)) #len of the target vocab dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5b26f0e-affd-448f-b65e-ff25db1111cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source text:\n",
      " politicians do not have permission to do what needs to be done\n",
      "numericalized text:\n",
      " tensor([   1, 2841,   69,   25,   29, 1616,    7,   69,   66,  570,    7,   21,\n",
      "         245,    2])\n"
     ]
    }
   ],
   "source": [
    "#let us check out the numericalized texts\n",
    "print('source text:\\n', lang_dataset.source_texts[0])\n",
    "print('numericalized text:\\n', lang_dataset[0][0]) #lang_dataset[0]-> souce numericalized and lang_dataset[1]-> target numericalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77ef0f04-f1b8-4138-93ed-5bc64b0df111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index of politicians:  2841\n"
     ]
    }
   ],
   "source": [
    "#let us confirm the the indexes match in the vocab\n",
    "print('index of politicians: ', lang_dataset.source_vocab.stoi['politicians'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced81e1-0597-4bdc-9633-5e70a4c488cc",
   "metadata": {},
   "source": [
    "* We can see the index for politicians matches to the vocab. \n",
    "* Also note that 1 -> start and 2 -> end is present in the numericalized texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10825572-74e6-44ba-b25c-268efac64e71",
   "metadata": {},
   "source": [
    "### 3. Create a class for adding padding to the batch\n",
    "\n",
    "* Dataset class returns the text one example at a time\n",
    "* DataLoader is used to get a batch from the dataset\n",
    "* Now, if we want to post-process a batch, we use dataloader's collate_fn. It applies a logic to the whole batch. We'll use this to 0 pad the batchs.\n",
    "  So, if a the max len in a batch is 12, the other sentences will be padded to be of length 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4988cc06-0951-4220-952a-8c54540294e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a class to create padding to the batches\n",
    "## collat_fn in dataloader is used for postprocessing on a single batch. Like __getitem__ in dataset class was used\n",
    "## for a single example\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    # __call__ :\n",
    "    ##    First the obj is created using MyCollate(pad_idx).\n",
    "    ##    Then if obj(batch) is called -> __call__ runs by default\n",
    "    ## https://www.geeksforgeeks.org/__call__-in-python/\n",
    "    def __call__(self, batch):\n",
    "        source = [item[0] for item in batch] #get the source lists\n",
    "        source = pad_sequence(source, batch_first=False, padding_value = self.pad_idx)\n",
    "        target = [item[1] for item in batch]\n",
    "        target = pad_sequence(target, batch_first=False, padding_value = self.pad_idx)\n",
    "        \n",
    "        return source,target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ed141-2c5a-431b-9818-517237dcb0e9",
   "metadata": {},
   "source": [
    "### 4. Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9c85136-f6bf-4504-b9f9-645aeb57053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get batches of data from dataset and pad them\n",
    "def get_loader(dataset, transform, batch_size, num_workers=1, shuffle=True, pin_memory=False):\n",
    "    pad_idx = dataset.source_vocab.stoi['<PAD>']\n",
    "    loader = DataLoader(dataset, batch_size = batch_size,# num_workers = num_workers,\n",
    "                        shuffle=shuffle,\n",
    "                       pin_memory=pin_memory, collate_fn = MyCollate(pad_idx=pad_idx))\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35165cb-f303-410a-9981-28d68a148aa5",
   "metadata": {},
   "source": [
    "> the number of samples we get from a loader at a time = batch_size * (num_workers+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04a63686-e001-4d30-a665-48759d757325",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_loader(lang_dataset,False, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ce754e1-33dc-4717-8bdb-b5e6d73646d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  tensor([[   1,    1,    1,  ...,    1,    1,    1],\n",
      "        [  54,   15,    4,  ...,   36,   17, 9702],\n",
      "        [  86, 5243,  113,  ...,  579,  175,    8],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    5],\n",
      "        [   0,    0,    0,  ...,    0,    0, 1615],\n",
      "        [   0,    0,    0,  ...,    0,    0,    2]])\n",
      "batch shape:  torch.Size([69, 64])\n"
     ]
    }
   ],
   "source": [
    "#batch[0] -> source batch\n",
    "#batch[1] -> target batch\n",
    "print('batch: ', batch[0])\n",
    "print('batch shape: ', batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ba9f1-40ba-4891-8bd9-10733079a200",
   "metadata": {},
   "source": [
    "* The shape of the batch is (length of longest sentence in batch, batch_size)\n",
    "* 1st Column is 1st sentence \n",
    "* We can see the \"0\" for padding smaller sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010b520-b664-4620-8a01-cc9c9cb0fe9f",
   "metadata": {},
   "source": [
    "#### The above code can be used for creating a custom dataset. We can reuse the same code for Image->Caption as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43d5fbeb-f186-40a4-871d-c3dc5778fbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   1, 2841,   69,   25,   29, 1616,    7,   69,   66,  570,    7,   21,\n",
       "          245,    2]),\n",
       " tensor([  1,   3,   4, 120,  24, 168,  75, 110,  37,  30,  14, 736,  18,   6,\n",
       "           2]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694da59-0258-48b2-8852-50af34b236f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
