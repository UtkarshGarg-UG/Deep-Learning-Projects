{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83dca0d2-4493-43d0-8cc8-b1cb48b33b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "from string import digits\n",
    "\n",
    "import os\n",
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58464b6-9f99-4b63-ab90-933a088c1d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b067295-318d-402e-9244-a85ad2c293c3",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e65c268-b6d7-404e-bb80-c7ba1e6dc0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Hindi_English_Truncated_Corpus.csv')\n",
    "data = data.reset_index(drop=True)\n",
    "data.drop('source',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3959e77-666e-4d42-9e41-1af641f7749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to tell you about one such child,</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what we really mean is that they're bad at not...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.The ending portion of these Vedas is called U...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  politicians do not have permission to do what ...   \n",
       "1         I'd like to tell you about one such child,   \n",
       "2  This percentage is even greater than the perce...   \n",
       "3  what we really mean is that they're bad at not...   \n",
       "4  .The ending portion of these Vedas is called U...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
       "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
       "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde03a1e-235b-4e46-8955-9ea04fdc7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocessing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819653c7-735f-4116-8602-b7a1131031d9",
   "metadata": {},
   "source": [
    "### 2. Create Dataset and Dataloaders\n",
    "* We create a dataset for train. \n",
    "* The vocab will only be based upon train.\n",
    "* We'll have to write separate prediction code for val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479f16aa-9fd7-4d44-823a-5b018b3ac3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4caf214b-e429-46b9-b8d0-f2ca10d4b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split_idx = int(len(data)*val_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26653caf-b138-4b12-a573-29adaf7215c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_idx = list(range(len(data)))\n",
    "np.random.shuffle(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f1c58a-935f-4976-a6eb-6e23fcd1b250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train:  112343\n",
      "len of val:  12482\n"
     ]
    }
   ],
   "source": [
    "val_idx, train_idx = data_idx[:val_split_idx], data_idx[val_split_idx:]\n",
    "print('len of train: ', len(train_idx))\n",
    "print('len of val: ', len(val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f23c72ae-e0ab-4e75-9f8b-dd85d2978d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.iloc[train_idx].reset_index().drop('index',axis=1)\n",
    "val = data.iloc[val_idx].reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca598f-b95e-4acc-bcd1-49839cf0ba78",
   "metadata": {},
   "source": [
    "### 3.Seq to Seq modelling\n",
    "#### Strategy\n",
    "* We'll create the main vocab using train and create train_dataset\n",
    "* We'll use the train vocab for val and create val_dataset as we want to see how the model handles oov words\n",
    "* We'll not touch the test set. We'll use this set like in production. We'll pass one test sentence to the model and it will give us a translated sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9351be2e-9647-4291-a639-3eb72200d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parameters for Seq2Seq model ###\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "#load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = 10000\n",
    "input_size_decoder = 10000 \n",
    "output_size = input_size_decoder #compulsory to define\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "#writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "\n",
    "transforms = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dabb6187-b941-4f70-b654-79b42975322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_layer import Train_Dataset, Validation_Dataset, get_train_loader, get_valid_loader\n",
    "\n",
    "train_dataset = Train_Dataset(train, 'english_sentence', 'hindi_sentence', source_vocab_max_size=input_size_encoder, target_vocab_max_size=input_size_decoder)\n",
    "val_dataset = Validation_Dataset(train_dataset, val, 'english_sentence', 'hindi_sentence')\n",
    "\n",
    "train_loader = get_train_loader(train_dataset, batch_size, num_workers=0, \n",
    "                          shuffle=True, pin_memory=True)\n",
    "val_loader = get_valid_loader(val_dataset, train_dataset, batch_size, num_workers=0, \n",
    "                          shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cb7cb-bb32-42df-9aa7-a24b44027209",
   "metadata": {},
   "source": [
    "#### And Experiment\n",
    "We try to see how much padding on average we have to give if we create random batches. Creating random batches of larger sizes lead to inefficient padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f2368f-5b80-4201-83d9-5cb31ea4367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_src_padding_list = []\n",
    "loader_trg_padding_list = []\n",
    "for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "    loader_src_padding_list.append(src.shape[0])\n",
    "    loader_trg_padding_list.append(trg.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1e89903-e6f1-44ce-81e9-114a516136ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:  [ 29  30  32 ... 350 371 400]\n",
      "trg:  [ 35  35  35 ... 314 419 419]\n"
     ]
    }
   ],
   "source": [
    "print('src: ' , np.sort(loader_src_padding_list))\n",
    "print('trg: ' , np.sort(loader_trg_padding_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14007794-b313-48e5-969c-a91224b70504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src avg:  72\n",
      "trg avg:  86\n"
     ]
    }
   ],
   "source": [
    "print('src avg: ', np.mean(loader_src_padding_list).astype(int))\n",
    "print('trg avg: ', np.mean(loader_trg_padding_list).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633918e9-b3c9-460b-8b49-937855f3adcb",
   "metadata": {},
   "source": [
    "### 4. Define Encoder, Decoder and Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "044b5bdb-2e8c-402f-860c-3623cdf7e5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(10000, 300)\n",
       "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(10000, 300)\n",
       "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=1024, out_features=10000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from seq_to_seq import Encoder, Decoder, Seq2Seq\n",
    "\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net, output_size, device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0c4f4b8-e5f7-4b13-bb45-acd1dab9bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "pad_idx = train_dataset.source_vocab.stoi['<PAD>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "145947d5-1153-41e0-ad6e-254a705d8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd457256-a7d6-4b96-8434-16d88a891edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import function to test on sentence\n",
    "from prediction import test_on_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fb635-d425-4083-afca-71f5103943f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss=0\n",
    "    valid_loss =0\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "    model.train()\n",
    "    for batch_idx, (src, trg) in enumerate(train_loader):\n",
    "        #print(batch_idx)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = src.to(device)\n",
    "        target = trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "        train_loss+= ((1 / (batch_idx + 1)) * (loss.data.item() - train_loss))\n",
    "        if batch_idx%100==0:\n",
    "            print('Avg train loss for last {} steps: {:.2f}'.format(batch_idx, train_loss))\n",
    "            print(test_on_sentence('and the doctors told his parents', model, train_dataset, device, max_len = 20))\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx, (src, trg) in enumerate(val_loader):\n",
    "        #print(batch_idx)\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = src.to(device)\n",
    "        target = trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        valid_loss+= ((1 / (batch_idx + 1)) * (loss.data.item() - valid_loss))\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9e5ed-73c5-443e-b4ba-fe767cf63ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
