{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documented-omega",
   "metadata": {},
   "source": [
    "### In this notebook we understand the implementation of forward hooks to get output activations from intermediate layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-pointer",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forced-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-authority",
   "metadata": {},
   "source": [
    "### 2. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "israeli-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(OrderedDict([('fc1',nn.Linear(128,64)),\n",
    "                       ('sigmoid', nn.Sigmoid()),\n",
    "                       ('fc2', nn.Linear(64,5)),\n",
    "                       ('softmax',nn.Softmax(dim=1))]))\n",
    "#define intput\n",
    "x = torch.rand(1, 128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-republican",
   "metadata": {},
   "source": [
    "### 3. Define hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interior-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {} #this will store the outputs of layers\n",
    "\n",
    "### This function returns the activations\n",
    "def get_activation(name): #name -> layer name. As we cannot send name to activation_hook directly we create an abstraciton to send name\n",
    "    def activation_hook(inst, inp, out):\n",
    "        \"\"\"Run activation hook.\n",
    "        Parameters\n",
    "        ----------\n",
    "        inst : torch.nn.Module\n",
    "            The layer we want to attach the hook to.\n",
    "        inp : tuple of torch.Tensor\n",
    "            The input to the `forward` method.\n",
    "        out : torch.Tensor\n",
    "            The output of the `forward` method.\n",
    "        \"\"\"\n",
    "        activations[name] = out.data\n",
    "    return activation_hook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-bibliography",
   "metadata": {},
   "source": [
    "#### 3.1 Method 1: Get layer by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expired-johnson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "terminal-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register forward hooks for the layers required\n",
    "model.fc1.register_forward_hook(get_activation('fc1'))\n",
    "model.sigmoid.register_forward_hook(get_activation('sigmoid'))\n",
    "model.fc2.register_forward_hook(get_activation('fc2'))\n",
    "model.softmax.register_forward_hook(get_activation('softmax'))\n",
    "\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "systematic-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1  :  torch.Size([1, 64])\n",
      "sigmoid  :  torch.Size([1, 64])\n",
      "fc2  :  torch.Size([1, 5])\n",
      "softmax  :  torch.Size([1, 5])\n",
      "  :  torch.Size([1, 5])\n",
      "\n",
      "softmax output:  tensor([[0.1325, 0.2931, 0.2387, 0.1624, 0.1733]])\n"
     ]
    }
   ],
   "source": [
    "for name, outputs in activations.items():\n",
    "    print(name,' : ', outputs.shape)\n",
    "print('\\nsoftmax output: ', activations['softmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hairy-sailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1325, 0.2931, 0.2387, 0.1624, 0.1733]])\n"
     ]
    }
   ],
   "source": [
    "print(y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-novelty",
   "metadata": {},
   "source": [
    "> We can see above that the forward hook for softmax returns the same values as output from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-ethernet",
   "metadata": {},
   "source": [
    "#### 3.2 Method 2: Get all layers by using model.named_modules()\n",
    "We can get name and the layers using named_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mechanical-programmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " : Sequential(\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "fc1 : Linear(in_features=128, out_features=64, bias=True)\n",
      "sigmoid : Sigmoid()\n",
      "fc2 : Linear(in_features=64, out_features=5, bias=True)\n",
      "softmax : Softmax(dim=1)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name, ':', module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "protective-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {} #this will store the outputs of layers\n",
    "#register forward hooks\n",
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(get_activation(name))\n",
    "\n",
    "\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-petersburg",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "confidential-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1  :  torch.Size([1, 64])\n",
      "sigmoid  :  torch.Size([1, 64])\n",
      "fc2  :  torch.Size([1, 5])\n",
      "softmax  :  torch.Size([1, 5])\n",
      "  :  torch.Size([1, 5])\n",
      "\n",
      "softmax output:  tensor([[0.1325, 0.2931, 0.2387, 0.1624, 0.1733]])\n"
     ]
    }
   ],
   "source": [
    "for name, outputs in activations.items():\n",
    "    print(name,' : ', outputs.shape)\n",
    "print('\\nsoftmax output: ', activations['softmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "explicit-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc1': tensor([[ 0.4593,  0.1147,  0.2386,  0.2440, -0.0586, -0.1439, -0.2947, -0.1832,\n",
       "           0.0562, -0.3983, -0.3785, -0.2134,  0.3738,  0.5855,  0.1553,  0.4979,\n",
       "          -0.3141,  0.1874, -0.1940,  0.1261,  0.0372,  0.0670, -0.3955,  0.1895,\n",
       "           0.4422, -0.0744, -0.0892,  0.4032,  0.1033, -0.2076,  0.1923,  0.2056,\n",
       "          -0.5855,  0.1144,  0.1287,  0.5942, -0.4269,  0.2187,  0.0882,  0.3308,\n",
       "           0.4109, -0.2804, -0.2610, -0.0395,  0.6415,  0.5011, -0.2736, -0.1263,\n",
       "          -0.3721, -0.2963,  0.2845,  0.0195,  0.1062,  0.1703, -0.3771, -0.1242,\n",
       "           0.0838,  0.3291,  0.1005, -0.3307, -0.1236,  0.1868,  0.3189,  0.6751]]),\n",
       " 'sigmoid': tensor([[0.6128, 0.5287, 0.5594, 0.5607, 0.4854, 0.4641, 0.4268, 0.4543, 0.5140,\n",
       "          0.4017, 0.4065, 0.4469, 0.5924, 0.6423, 0.5387, 0.6220, 0.4221, 0.5467,\n",
       "          0.4516, 0.5315, 0.5093, 0.5167, 0.4024, 0.5472, 0.6088, 0.4814, 0.4777,\n",
       "          0.5995, 0.5258, 0.4483, 0.5479, 0.5512, 0.3577, 0.5286, 0.5321, 0.6443,\n",
       "          0.3949, 0.5545, 0.5220, 0.5820, 0.6013, 0.4304, 0.4351, 0.4901, 0.6551,\n",
       "          0.6227, 0.4320, 0.4685, 0.4080, 0.4265, 0.5707, 0.5049, 0.5265, 0.5425,\n",
       "          0.4068, 0.4690, 0.5209, 0.5815, 0.5251, 0.4181, 0.4692, 0.5466, 0.5791,\n",
       "          0.6627]]),\n",
       " 'fc2': tensor([[-0.2604,  0.5340,  0.3286, -0.0567,  0.0084]]),\n",
       " 'softmax': tensor([[0.1325, 0.2931, 0.2387, 0.1624, 0.1733]]),\n",
       " '': tensor([[0.1325, 0.2931, 0.2387, 0.1624, 0.1733]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "friendly-equation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1325, 0.2931, 0.2387, 0.1624, 0.1733]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-consultancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-rating",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
